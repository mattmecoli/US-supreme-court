{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Set styling parameters\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from poibin import PoiBin\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ML models \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = pd.read_csv('SCDB_2018_01_caseCentered_Citation.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>ledCite</th>\n",
       "      <th>lexisCite</th>\n",
       "      <th>...</th>\n",
       "      <th>authorityDecision1</th>\n",
       "      <th>authorityDecision2</th>\n",
       "      <th>lawType</th>\n",
       "      <th>lawSupp</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946-001</td>\n",
       "      <td>1946-001-01</td>\n",
       "      <td>1946-001-01-01</td>\n",
       "      <td>1946-001-01-01-01</td>\n",
       "      <td>11/18/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 1</td>\n",
       "      <td>67 S. Ct. 6</td>\n",
       "      <td>91 L. Ed. 3</td>\n",
       "      <td>1946 U.S. LEXIS 1724</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>35 U.S.C. § 33</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946-002</td>\n",
       "      <td>1946-002-01</td>\n",
       "      <td>1946-002-01-01</td>\n",
       "      <td>1946-002-01-01-01</td>\n",
       "      <td>11/18/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 14</td>\n",
       "      <td>67 S. Ct. 13</td>\n",
       "      <td>91 L. Ed. 12</td>\n",
       "      <td>1946 U.S. LEXIS 1725</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>18 U.S.C. § 398</td>\n",
       "      <td>81.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946-003</td>\n",
       "      <td>1946-003-01</td>\n",
       "      <td>1946-003-01-01</td>\n",
       "      <td>1946-003-01-01-01</td>\n",
       "      <td>11/18/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 29</td>\n",
       "      <td>67 S. Ct. 1</td>\n",
       "      <td>91 L. Ed. 22</td>\n",
       "      <td>1946 U.S. LEXIS 3037</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946-004</td>\n",
       "      <td>1946-004-01</td>\n",
       "      <td>1946-004-01-01</td>\n",
       "      <td>1946-004-01-01-01</td>\n",
       "      <td>11/25/1946</td>\n",
       "      <td>7</td>\n",
       "      <td>329 U.S. 40</td>\n",
       "      <td>67 S. Ct. 167</td>\n",
       "      <td>91 L. Ed. 29</td>\n",
       "      <td>1946 U.S. LEXIS 1696</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49 Stat. 801</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946-005</td>\n",
       "      <td>1946-005-01</td>\n",
       "      <td>1946-005-01-01</td>\n",
       "      <td>1946-005-01-01-01</td>\n",
       "      <td>11/25/1946</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 64</td>\n",
       "      <td>67 S. Ct. 154</td>\n",
       "      <td>91 L. Ed. 44</td>\n",
       "      <td>1946 U.S. LEXIS 2997</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     caseId     docketId    caseIssuesId             voteId dateDecision  \\\n",
       "0  1946-001  1946-001-01  1946-001-01-01  1946-001-01-01-01   11/18/1946   \n",
       "1  1946-002  1946-002-01  1946-002-01-01  1946-002-01-01-01   11/18/1946   \n",
       "2  1946-003  1946-003-01  1946-003-01-01  1946-003-01-01-01   11/18/1946   \n",
       "3  1946-004  1946-004-01  1946-004-01-01  1946-004-01-01-01   11/25/1946   \n",
       "4  1946-005  1946-005-01  1946-005-01-01  1946-005-01-01-01   11/25/1946   \n",
       "\n",
       "   decisionType       usCite        sctCite       ledCite  \\\n",
       "0             1   329 U.S. 1    67 S. Ct. 6   91 L. Ed. 3   \n",
       "1             1  329 U.S. 14   67 S. Ct. 13  91 L. Ed. 12   \n",
       "2             1  329 U.S. 29    67 S. Ct. 1  91 L. Ed. 22   \n",
       "3             7  329 U.S. 40  67 S. Ct. 167  91 L. Ed. 29   \n",
       "4             1  329 U.S. 64  67 S. Ct. 154  91 L. Ed. 44   \n",
       "\n",
       "              lexisCite    ...     authorityDecision1  authorityDecision2  \\\n",
       "0  1946 U.S. LEXIS 1724    ...                    4.0                 NaN   \n",
       "1  1946 U.S. LEXIS 1725    ...                    4.0                 NaN   \n",
       "2  1946 U.S. LEXIS 3037    ...                    1.0                 NaN   \n",
       "3  1946 U.S. LEXIS 1696    ...                    4.0                 NaN   \n",
       "4  1946 U.S. LEXIS 2997    ...                    7.0                 NaN   \n",
       "\n",
       "  lawType lawSupp         lawMinor majOpinWriter majOpinAssigner  splitVote  \\\n",
       "0     6.0   600.0   35 U.S.C. § 33          78.0            78.0          1   \n",
       "1     6.0   600.0  18 U.S.C. § 398          81.0            87.0          1   \n",
       "2     2.0   207.0              NaN          84.0            78.0          1   \n",
       "3     6.0   600.0     49 Stat. 801          87.0            87.0          1   \n",
       "4     NaN     NaN              NaN          78.0            87.0          1   \n",
       "\n",
       "   majVotes  minVotes  \n",
       "0         8         1  \n",
       "1         6         3  \n",
       "2         5         4  \n",
       "3         5         3  \n",
       "4         6         3  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv('SCDB_2018_01_justiceCentered_Citation.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "og_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = og_data.drop(columns = ['justice', 'docketId', 'caseIssuesId', 'voteId', 'dateDecision',\n",
    "                                        'usCite', 'sctCite', 'ledCite', 'lexisCite', \n",
    "                                        'docket', 'caseName', 'petitionerState', 'respondentState', \n",
    "                                        'adminActionState', 'caseOriginState',\n",
    "                                        'caseSourceState', 'declarationUncon', \n",
    "                                        'caseDispositionUnusual', 'partyWinning', 'voteUnclear',\n",
    "                                        'decisionDirectionDissent', 'authorityDecision1', 'authorityDecision2',\n",
    "                                        'lawMinor', 'majOpinWriter', 'majOpinAssigner',\n",
    "                                        'splitVote','firstAgreement', 'secondAgreement', \n",
    "                                        'dateArgument', 'dateRearg',\n",
    "                                        'term', 'caseDisposition', 'decisionDirection',\n",
    "                                        'majVotes', 'minVotes', 'majority', 'vote', 'opinion', \n",
    "                                       'precedentAlteration', 'issueArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = working_data.dropna(subset=['direction'])\n",
    "working_data = working_data.fillna(int(999))\n",
    "d = {1: 0, 2: 1}\n",
    "working_data['direction'] = working_data['direction'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_to_dummy = ['caseId', 'justiceName', 'direction']\n",
    "wd_columns_to_dummy = list(working_data.columns)\n",
    "for n in not_to_dummy:\n",
    "    wd_columns_to_dummy.remove(n)\n",
    "wd_columns_to_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = pd.get_dummies(working_data, columns = wd_columns_to_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.justiceName.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data['direction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just issue areas (for the sake of dimensionality, to see if there's any true value to keeping all those categories). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_issue_data = working_data.drop(columns = ['issue'])\n",
    "no_issue_data = working_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ginsburg_df = no_issue_data[no_issue_data['justiceName'] == 'RBGinsburg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Ginsburg_df[Ginsburg_df['adminAction'].notnull()]:\n",
    "#     Ginsburg_df['lcDisposition'] = 4\n",
    "    \n",
    "Ginsburg_df = Ginsburg_df.dropna(subset=['direction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = Ginsburg_df['caseId']\n",
    "Ginsburg_df = Ginsburg_df.drop(columns = ['caseId', 'justiceName'])\n",
    "# df = df[pd.notnull(df['EPS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ginsburg_target = Ginsburg_df['direction']\n",
    "Ginsburg_df = Ginsburg_df.drop(columns = ['direction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test, case_train, case_test = train_test_split(Ginsburg_df, Ginsburg_target, case_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(oob_score = True, n_estimators=200, max_depth = 30)\n",
    "forest.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On Training, score was {}\".format(forest.score(data_train, target_train)))\n",
    "print(\"On Test, score was {}\".format(forest.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = forest.predict_proba(data_test)\n",
    "roc_auc_score(target_test, probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_series = pd.DataFrame(data = probs[:,1], index = case_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_series = pd.DataFrame(data = data_test['direction'].values, index = working_test_cases.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_with_ids = pd.concat([probs_series, case_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs_with_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_with_ids.rename(columns={0:'probability'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_with_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justices = ['RBG', 'Alito', 'Souter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_probas = pd.DataFrame(columns = justices, index = case_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in probs_with_ids.iterrows():\n",
    "    case = row['caseId']\n",
    "    probabil = row['probability']\n",
    "    master_probas['RBG'].loc[case] = probabil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_probas = master_probas.fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict.fromkeys(list(master_probas.index.values), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in master_probas.iterrows():\n",
    "    lista = []\n",
    "    for c in master_probas.columns:\n",
    "        if row[c] != 2:\n",
    "            lista.append(row[c])\n",
    "    lista = lista + [0.7, 0.5, 0.66, 0.44, 0.9, 0.83, 0.7, 0.71]\n",
    "    ps[ind] = lista "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = {}\n",
    "x = [5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ps.keys():\n",
    "    if len(ps[k]) == 9:\n",
    "        pb = PoiBin(ps[k])\n",
    "        outcomes[k] = sum(pb.pmf(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(forest.predict(data_test), target_test)\n",
    "print('Confusion Matrix:\\n',cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "#Add title and Axis Labels\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Add appropriate Axis Scales\n",
    "class_names = ['conservative', 'liberal'] #Get class labels to add to matrix\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "#Add Labels to Each Cell\n",
    "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "#Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "#Add a Side Bar Legend Showing Colors\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(oob_score = True, n_estimators=1500, max_depth = 30)\n",
    "forest.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to do hyperparameter tuning, note that this takes a while to run and will tax your system\n",
    "# a bit if you're running this on a laptop. \n",
    "\n",
    "# n_estimators = [300, 500, 700]\n",
    "# max_depth = [10,12,14,16]\n",
    "\n",
    "# param_grid_forest = dict(n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "# grid_search = GridSearchCV(forest, param_grid_forest, scoring=\"accuracy\", n_jobs=-1, cv=kfold)\n",
    "# grid_result = grid_search.fit(data_train, target_train)\n",
    "\n",
    "# # Interpreting results \n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "model_log = logreg.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model_log.predict_proba(data_test)\n",
    "roc_auc_score(target_test, probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"On Training, score was {}\".format(model_log.score(data_train, target_train)))\n",
    "print(\"On Test, score was {}\".format(model_log.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(model_log.predict(data_test), target_test)\n",
    "print('Confusion Matrix:\\n',cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "#Add title and Axis Labels\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Add appropriate Axis Scales\n",
    "class_names = ['conservative', 'liberal'] #Get class labels to add to matrix\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "#Add Labels to Each Cell\n",
    "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "#Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "#Add a Side Bar Legend Showing Colors\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_set = [(data_test, target_test)]\n",
    "# xgb_model = xgb.XGBClassifier()\n",
    "# xgb_model.fit(data_train, target_train, early_stopping_rounds=30, eval_metric=\"error\", eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"On Training, score was {}\".format(xgb_model.score(data_train, target_train)))\n",
    "# print(\"On Test, score was {}\".format(xgb_model.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to do hyperparameter tuning, note that this takes a while to run and will tax your system\n",
    "# a bit if you're running this on a laptop. \n",
    "\n",
    "# learning_rate = [0.005, 0.01, 0.02]\n",
    "# n_estimators = [150]\n",
    "# max_depth = [2,4,6]\n",
    "\n",
    "# param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "# grid_search = GridSearchCV(xgb_model, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold)\n",
    "# grid_result = grid_search.fit(data_train, target_train)\n",
    "\n",
    "# # Interpreting results \n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "nb = GaussianNB()\n",
    "nbmodel = nb.fit(data_train, target_train)\n",
    "nbpreds = nbmodel.predict(data_test)\n",
    "accuracy = accuracy_score(target_test, nbpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "sv = svm.SVC(kernel='rbf', probability = True, random_state = 124)\n",
    "svmmodel = sv.fit(data_train, target_train)\n",
    "svmpreds = svmmodel.predict(data_test)\n",
    "accuracy_score(target_test, svmpreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
